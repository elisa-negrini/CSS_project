{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "client_id = \"XpeJWmueFBEGkyXjE-dpcA\"\n",
    "client_secret = \"yOyCLHaB0Ur7R0sW75UUmc20MjCPkw\"\n",
    "user_agent = \"Samu_Miki\"\n",
    "username = \"Samu_Miki\"\n",
    "password = \"M!rand0la!\"\n",
    "subreddit = 'politics'\n",
    "query = 'russian AND invasion'\n",
    "comment_score_min = 0\n",
    "output_json = 'russian_invasion_rel_p.json'\n",
    "\n",
    "# --- PAGINAZIONE SETTINGS ---\n",
    "max_pages = 50  # Numero massimo di pagine da recuperare (100 posts per pagina = 5000 posts max)\n",
    "request_delay = 1.5  # Secondi tra le richieste\n",
    "limit_per_page = 1000  # Reddit max per request\n",
    "\n",
    "# --- Step 1: OAuth2 Token ---\n",
    "print(\"üîê Ottenendo token di accesso...\")\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "data = {'grant_type': 'password', 'username': username, 'password': password}\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "try:\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "    token = res.json()['access_token']\n",
    "    headers['Authorization'] = f'bearer {token}'\n",
    "    print(\"‚úÖ Token ottenuto con successo!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore nell'ottenere il token: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Helper: Recursive Comment Extractor ---\n",
    "def extract_comments(children, threshold):\n",
    "    results = []\n",
    "    for child in children:\n",
    "        kind = child.get('kind')\n",
    "        data = child.get('data', {})\n",
    "        if kind != 't1':\n",
    "            continue\n",
    "        score = data.get('score', 0)\n",
    "        body = data.get('body', '')\n",
    "        if score >= threshold and not body.lower().startswith('[deleted') and body.strip():\n",
    "            results.append({\n",
    "                'author': data.get('author'),\n",
    "                'score': score,\n",
    "                'body': body,\n",
    "                'created_utc': datetime.utcfromtimestamp(data['created_utc']).isoformat()\n",
    "            })\n",
    "        replies = data.get('replies')\n",
    "        if replies and isinstance(replies, dict):\n",
    "            results.extend(extract_comments(replies['data']['children'], threshold))\n",
    "    return results\n",
    "\n",
    "# --- Main Loop: Paginazione con \"after\" ---\n",
    "all_data = []\n",
    "total_posts = 0\n",
    "total_comments = 0\n",
    "current_page = 1\n",
    "after_token = None  # Token per la paginazione\n",
    "\n",
    "print(f\"\\nüìÑ Inizio raccolta dati con paginazione (max {max_pages} pagine)...\")\n",
    "print(f\"üîç Query: '{query}' in r/{subreddit}\")\n",
    "\n",
    "while current_page <= max_pages:\n",
    "    print(f\"\\nüìÉ Pagina {current_page}/{max_pages}...\")\n",
    "    \n",
    "    # Parametri per la ricerca\n",
    "    search_url = f'https://oauth.reddit.com/r/{subreddit}/search'\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'limit': limit_per_page,\n",
    "        'sort': 'relevance',  # Manteniamo relevance come richiesto\n",
    "        'restrict_sr': True\n",
    "    }\n",
    "    \n",
    "    # Aggiungi after token se disponibile (per paginazione)\n",
    "    if after_token:\n",
    "        params['after'] = after_token\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(search_url, headers=headers, params=params)\n",
    "        \n",
    "        if resp.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Errore API: {resp.status_code}\")\n",
    "            print(f\"Response: {resp.text}\")\n",
    "            break\n",
    "            \n",
    "        data = resp.json()\n",
    "        posts = data['data']['children']\n",
    "        after_token = data['data'].get('after')  # Token per la prossima pagina\n",
    "        \n",
    "        print(f\"   üìù Trovati {len(posts)} posts in questa pagina\")\n",
    "        \n",
    "        # Se non ci sono pi√π posts, fermiamoci\n",
    "        if not posts:\n",
    "            print(\"   ‚ÑπÔ∏è Nessun post trovato, fine raccolta\")\n",
    "            break\n",
    "            \n",
    "        # Se non c'√® un after token, siamo all'ultima pagina\n",
    "        if not after_token:\n",
    "            print(\"   ‚ÑπÔ∏è Ultima pagina raggiunta (nessun 'after' token)\")\n",
    "        \n",
    "        page_posts = 0\n",
    "        page_comments = 0\n",
    "        \n",
    "        # Processa ogni post della pagina\n",
    "        for post in posts:\n",
    "            post_data = post['data']\n",
    "            post_id = post_data['id']\n",
    "            title = post_data['title']\n",
    "            selftext = post_data.get('selftext', '')\n",
    "            score = post_data['score']\n",
    "            author = post_data.get('author', '[deleted]')\n",
    "            created_utc = datetime.utcfromtimestamp(post_data['created_utc']).isoformat()\n",
    "            num_comments = post_data['num_comments']\n",
    "            \n",
    "            # Fetch comments per questo post\n",
    "            comment_url = f'https://oauth.reddit.com/comments/{post_id}.json'\n",
    "            try:\n",
    "                response = requests.get(comment_url, headers=headers, params={'depth': 10, 'limit': 500})\n",
    "                if response.status_code == 200:\n",
    "                    comment_blob = response.json()[1]['data']['children']\n",
    "                    high_comments = extract_comments(comment_blob, comment_score_min)\n",
    "                else:\n",
    "                    print(f\"      ‚ö†Ô∏è Errore {response.status_code} per comments post {post_id}\")\n",
    "                    high_comments = []\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Errore comments per post {post_id}: {e}\")\n",
    "                high_comments = []\n",
    "            \n",
    "            # Costruisci record\n",
    "            post_record = {\n",
    "                'page_number': current_page,\n",
    "                'post_id': post_id,\n",
    "                'title': title,\n",
    "                'author': author,\n",
    "                'score': score,\n",
    "                'created_utc': created_utc,\n",
    "                'selftext': selftext,\n",
    "                'num_comments': num_comments,\n",
    "                'high_score_comments': high_comments\n",
    "            }\n",
    "            \n",
    "            all_data.append(post_record)\n",
    "            page_posts += 1\n",
    "            page_comments += len(high_comments)\n",
    "            \n",
    "            # Piccola pausa tra i posts\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        total_posts += page_posts\n",
    "        total_comments += page_comments\n",
    "        \n",
    "        print(f\"   ‚úÖ {page_posts} posts, {page_comments} commenti raccolti da pagina {current_page}\")\n",
    "        print(f\"   üìä Totale finora: {total_posts} posts, {total_comments} commenti\")\n",
    "        \n",
    "        # Se non c'√® after token, usciamo dal loop\n",
    "        if not after_token:\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore generale pagina {current_page}: {e}\")\n",
    "        break\n",
    "    \n",
    "    # Incrementa pagina\n",
    "    current_page += 1\n",
    "    \n",
    "    # Pausa tra le richieste\n",
    "    time.sleep(request_delay)\n",
    "    \n",
    "    # Salva progressi ogni 10 pagine\n",
    "    if current_page % 10 == 0:\n",
    "        print(f\"üíæ Salvataggio progressi... ({total_posts} posts totali finora)\")\n",
    "        with open(f\"temp_{output_json}\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- Salvataggio finale ---\n",
    "print(f\"\\nüíæ Salvataggio finale...\")\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Statistiche finali\n",
    "print(f\"\\nüéâ COMPLETATO!\")\n",
    "print(f\"üìä Statistiche finali:\")\n",
    "print(f\"   üìÑ Pagine processate: {current_page - 1}\")\n",
    "print(f\"   üìù Posts totali: {total_posts}\")\n",
    "print(f\"   üí¨ Commenti totali: {total_comments}\")\n",
    "print(f\"   üìÑ File salvato: {output_json}\")\n",
    "if current_page > 1:\n",
    "    print(f\"   ‚è±Ô∏è Media posts/pagina: {total_posts/(current_page-1):.1f}\")\n",
    "\n",
    "# Rimuovi file temporaneo se esiste\n",
    "if os.path.exists(f\"temp_{output_json}\"):\n",
    "    os.remove(f\"temp_{output_json}\")\n",
    "    print(f\"üóëÔ∏è File temporaneo rimosso\")\n",
    "\n",
    "print(f\"\\n‚ú® Raccolta completata! Trovati dati da {len(set(record['post_id'] for record in all_data))} post unici.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e209f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Numero totale di post: 197\n",
      "üí¨ Numero totale di commenti ad alto punteggio: 17370\n",
      "\n",
      "üßæ Esempio del DataFrame:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "NOME_FILE = \"russian_invasion_rel_p\"\n",
    "\n",
    "# --- Caricamento del file JSON ---\n",
    "with open(f'{NOME_FILE}.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Statistiche ---\n",
    "num_posts = len(data)\n",
    "num_comments = sum(len(post['high_score_comments']) for post in data)\n",
    "\n",
    "print(f\"üìå Numero totale di post: {num_posts}\")\n",
    "print(f\"üí¨ Numero totale di commenti ad alto punteggio: {num_comments}\")\n",
    "\n",
    "# --- Creazione del DataFrame ---\n",
    "rows = []\n",
    "for post in data:\n",
    "    post_id = post['post_id']\n",
    "    post_title = post['title']\n",
    "    post_author = post['author']\n",
    "    post_score = post['score']\n",
    "    post_created_utc = post['created_utc']\n",
    "    \n",
    "    for comment in post['high_score_comments']:\n",
    "        rows.append({\n",
    "            'post_id': post_id,\n",
    "            'post_title': post_title,\n",
    "            'post_author': post_author,\n",
    "            'post_score': post_score,\n",
    "            'post_created_utc': post_created_utc,\n",
    "            'comment_author': comment['author'],\n",
    "            'comment_score': comment['score'],\n",
    "            'comment_body': comment['body'],\n",
    "            'comment_created_utc': comment['created_utc']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# --- Visualizzazione del DataFrame ---\n",
    "print(\"\\nüßæ Esempio del DataFrame:\")\n",
    "df.tail()\n",
    "\n",
    "# Se vuoi salvarlo anche come CSV:\n",
    "df.to_csv(f'{NOME_FILE}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
