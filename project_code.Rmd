---
title: "CSS code"
author: "Elisa Negrini"
date: "2025-07-28"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load datasets

LOADING VECCHIO
```{r}
data_new <- readr::read_csv("datasets/reddit_comments_expanded_new.csv", show_col_types = FALSE)
data_pag <- readr::read_csv("datasets/reddit_comments_expanded_paginated.csv", show_col_types = FALSE)

data_trump_new <- readr::read_csv("datasets/trump_new.csv", show_col_types = FALSE)
data_trump_rel <- readr::read_csv("datasets/trump_rel.csv", show_col_types = FALSE)
data_trump_zelensky_new <- read_csv("datasets/trump_zelensky_new.csv", show_col_types = FALSE)
data_trump_zelensky_rel <- read_csv("datasets/trump_zelensky_rel.csv", show_col_types = FALSE)
data_war_ukraine_new <- read_csv("datasets/war_ukraine_new.csv", show_col_types = FALSE)
data_war_ukraine_rel <- read_csv("datasets/war_ukraine_rel.csv", show_col_types = FALSE)
data_youtube_zelensky_trump_comments_only <- read_csv("datasets/youtube_zelensky_trump_comments_only.csv", show_col_types = FALSE)
data_youtube_zelensky_trump_data <- read_csv("datasets/youtube_zelensky_trump_data.csv", show_col_types = FALSE)
data_youtube_zelensky_trump_video_summary <- read_csv("datasets/youtube_zelensky_trump_video_summary.csv", show_col_types = FALSE)
data_zelensky_new <- read_csv("datasets/zelensky_new.csv", show_col_types = FALSE)
data_zelensky_rel <- read_csv("datasets/zelensky_rel.csv", show_col_types = FALSE)

```

LOADING NUOVO
```{r}
library(readr)

#reddit
data_putin_new_p <- readr::read_csv("dati/putin_new_p.csv", show_col_types = FALSE)
data_putin_new_u <- readr::read_csv("dati/putin_new_u.csv", show_col_types = FALSE)
data_putin_new_wn <- readr::read_csv("dati/putin_new_wn.csv", show_col_types = FALSE)
data_putin_rel_p <- readr::read_csv("dati/putin_rel_p.csv", show_col_types = FALSE)
data_putin_rel_u <- readr::read_csv("dati/putin_rel_u.csv", show_col_types = FALSE)
data_putin_rel_wn <- readr::read_csv("dati/putin_rel_wn.csv", show_col_types = FALSE)

data_russian_inv_new_p <- readr::read_csv("dati/russian_invasion_new_p.csv", show_col_types = FALSE)
data_russian_inv_new_u <- readr::read_csv("dati/russian_invasion_new_u.csv", show_col_types = FALSE)
data_russian_inv_new_wn <- readr::read_csv("dati/russian_invasion_new_wn.csv", show_col_types = FALSE)
data_russian_inv_rel_p <- readr::read_csv("dati/russian_invasion_rel_p.csv", show_col_types = FALSE)
data_russian_inv_rel_u <- readr::read_csv("dati/russian_invasion_rel_u.csv", show_col_types = FALSE)
data_russian_inv_rel_wn <- readr::read_csv("dati/russian_invasion_rel_wn.csv", show_col_types = FALSE)

data_trump_new_p <- readr::read_csv("dati/trump_new_p.csv", show_col_types = FALSE)
data_trump_new_u <- readr::read_csv("dati/trump_new_u.csv", show_col_types = FALSE)
data_trump_new_wn <- readr::read_csv("dati/trump_new_wn.csv", show_col_types = FALSE)
data_trump_rel_p <- readr::read_csv("dati/trump_rel_p.csv", show_col_types = FALSE)
data_trump_rel_u <- readr::read_csv("dati/trump_rel_u.csv", show_col_types = FALSE)
data_trump_rel_wn <- readr::read_csv("dati/trump_rel_wn.csv", show_col_types = FALSE)

data_trump_zelensky_new_p <- readr::read_csv("dati/trump_zelensky_new_p.csv", show_col_types = FALSE)
data_trump_zelensky_new_u <- readr::read_csv("dati/trump_zelensky_new_u.csv", show_col_types = FALSE)
data_trump_zelensky_new_wn <- readr::read_csv("dati/trump_zelensky_new_wn.csv", show_col_types = FALSE)
data_trump_zelensky_rel_p <- readr::read_csv("dati/trump_zelensky_rel_p.csv", show_col_types = FALSE)
data_trump_zelensky_rel_u <- readr::read_csv("dati/trump_zelensky_rel_u.csv", show_col_types = FALSE)
data_trump_zelensky_rel_wn <- readr::read_csv("dati/trump_zelensky_rel_wn.csv", show_col_types = FALSE)

data_war_ukraine_new_p <- readr::read_csv("dati/war_ukraine_new_p.csv", show_col_types = FALSE)
data_war_ukraine_new_u <- readr::read_csv("dati/war_ukraine_new_u.csv", show_col_types = FALSE)
data_war_ukraine_new_wn <- readr::read_csv("dati/war_ukraine_new_wn.csv", show_col_types = FALSE)
data_war_ukraine_rel_p <- readr::read_csv("dati/war_ukraine_rel_p.csv", show_col_types = FALSE)
data_war_ukraine_rel_u <- readr::read_csv("dati/war_ukraine_rel_u.csv", show_col_types = FALSE)
data_war_ukraine_rel_wn <- readr::read_csv("dati/war_ukraine_rel_wn.csv", show_col_types = FALSE)

data_zelensky_new_p <- readr::read_csv("dati/zelensky_new_p.csv", show_col_types = FALSE)
data_zelensky_new_u <- readr::read_csv("dati/zelensky_new_u.csv", show_col_types = FALSE)
data_zelensky_new_wn <- readr::read_csv("dati/zelensky_new_wn.csv", show_col_types = FALSE)
data_zelensky_rel_p <- readr::read_csv("dati/zelensky_rel_p.csv", show_col_types = FALSE)
data_zelensky_rel_u <- readr::read_csv("dati/zelensky_rel_u.csv", show_col_types = FALSE)
data_zelensky_rel_wn <- readr::read_csv("dati/zelensky_rel_wn.csv", show_col_types = FALSE)

# youtube
data_yt_war_ukraine1 <- readr::read_csv("dati/youtube_war_ukraine_data_1.csv", show_col_types = FALSE)
data_yt_war_ukraine2 <- readr::read_csv("dati/youtube_war_ukraine_data_2.csv", show_col_types = FALSE)
data_yt_war_ukraine1_comm <- readr::read_csv("dati/youtube_war_ukraine_comments_only_1.csv", show_col_types = FALSE)
data_yt_war_ukraine2_comm <- readr::read_csv("dati/youtube_war_ukraine_comments_only_2.csv", show_col_types = FALSE)
data_yt_zelensky_trump1 <- readr::read_csv("dati/youtube_zelensky_trump_data_1.csv", show_col_types = FALSE)
data_yt_zelensky_trump2 <- readr::read_csv("dati/youtube_zelensky_trump_data_2.csv", show_col_types = FALSE)
data_yt_zelensky_trump1_comm <- readr::read_csv("dati/youtube_zelensky_trump_comments_only_1.csv", show_col_types = FALSE)
data_yt_zelensky_trump2_comm <- readr::read_csv("dati/youtube_zelensky_trump_comments_only_2.csv", show_col_types = FALSE)
```

Variables for reddit datasets:

```{r}
names(data_new)
```
Variables for youtube datasets.
```{r}
names(data_youtube_zelensky_trump_comments_only)
```

## Exploratory analysis OLD

### 1. on separated datasets

#### Check users activity (posts + comments)

Function to visualize the activity (by post or comments) over time:

```{r}
library(dplyr)
library(lubridate)
library(ggplot2)
library(patchwork)

plot_activity_over_time <- function(dataset, source = "reddit") {
  
  dataset_name <- deparse(substitute(dataset))
  
  if (source == "reddit") {
    dataset <- dataset %>%
      mutate(
        comment_created = ymd_hms(comment_created_utc),
        post_created = ymd_hms(post_created_utc)
      )
    
    # Count comments per day
    comments_per_day <- dataset %>%
      mutate(date = as_date(comment_created)) %>%
      count(date, name = "count")
    
    # Count posts per day
    posts_per_day <- dataset %>%
      mutate(date = as_date(post_created)) %>%
      count(date, name = "count")
    
    # Plots
    p_comments <- ggplot(comments_per_day, aes(x = date, y = count)) +
      geom_line(color = "steelblue") +
      labs(
        title = paste("Comments Over Time -", dataset_name),
        x = "Date",
        y = "Number of Comments"
      ) +
      theme_minimal()
    
    p_posts <- ggplot(posts_per_day, aes(x = date, y = count)) +
      geom_line(color = "darkred") +
      labs(
        title = paste("Posts Over Time -", dataset_name),
        x = "Date",
        y = "Number of Posts"
      ) +
      theme_minimal()
    
  } else if (source == "youtube") {
    dataset <- dataset %>%
      mutate(
        comment_created = ymd_hms(comment_published_at),
        video_created = ymd_hms(video_published_at)
      )
    
    # Count comments per day
    comments_per_day <- dataset %>%
      mutate(date = as_date(comment_created)) %>%
      count(date, name = "count")
    
    # Count videos per day
    videos_per_day <- dataset %>%
      mutate(date = as_date(video_created)) %>%
      count(date, name = "count")
    
    # Plots
    p_comments <- ggplot(comments_per_day, aes(x = date, y = count)) +
      geom_line(color = "steelblue") +
      labs(
        title = paste("YouTube Comments Over Time -", dataset_name),
        x = "Date",
        y = "Number of Comments"
      ) +
      theme_minimal()
    
    p_posts <- ggplot(videos_per_day, aes(x = date, y = count)) +
      geom_line(color = "darkred") +
      labs(
        title = paste("YouTube Videos Over Time -", dataset_name),
        x = "Date",
        y = "Number of Videos"
      ) +
      theme_minimal()
  }
  return(p_comments / p_posts + plot_layout(heights = c(1, 1)))
}
```

```{r}
plot_activity_over_time(data_new)
plot_activity_over_time(data_pag)
plot_activity_over_time(data_trump_new)
plot_activity_over_time(data_trump_rel)
plot_activity_over_time(data_trump_zelensky_new)
plot_activity_over_time(data_trump_zelensky_rel)
plot_activity_over_time(data_war_ukraine_new)
plot_activity_over_time(data_war_ukraine_rel)
plot_activity_over_time(data_youtube_zelensky_trump_comments_only, source = "youtube")
plot_activity_over_time(data_youtube_zelensky_trump_data, source = "youtube")
plot_activity_over_time(data_zelensky_new)
plot_activity_over_time(data_zelensky_rel)
```

To visualize the distribution on comments per post for both dataset the
boxplots are shown:

```{r}
library(dplyr)
library(ggplot2)
library(purrr)

plot_comments_per_item <- function(..., source = "reddit") {
  
  datasets <- list(...)
  dataset_names <- as.list(substitute(list(...)))[-1L] |> sapply(deparse)
  
  combined_data <- purrr::map2_dfr(datasets, dataset_names, ~ {
    df <- .x
    
    if (source == "reddit") {
      df %>%
        group_by(post_id) %>%
        summarise(n_comments = n(), .groups = "drop") %>%
        mutate(dataset = .y)
    } else {
      df %>%
        group_by(video_id) %>%
        summarise(n_comments = n(), .groups = "drop") %>%
        mutate(dataset = .y)
    }
  })
  
  ggplot(combined_data, aes(x = dataset, y = n_comments, fill = dataset)) +
    geom_boxplot() +
    labs(
      title = ifelse(source == "reddit",
                     "Number of Comments per Post by Dataset",
                     "Number of Comments per Video by Dataset"),
      x = "Dataset",
      y = "Number of Comments"
    ) +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}


plot_comments_per_item(data_trump_new, data_trump_rel, data_zelensky_new, data_zelensky_rel, data_trump_zelensky_new, data_trump_zelensky_rel, data_new, data_pag, data_war_ukraine_new, data_war_ukraine_rel)
plot_comments_per_item(data_youtube_zelensky_trump_comments_only, data_youtube_zelensky_trump_data, source = "youtube")
```

#### Frequent words - reddit

ON REDDIT The most frequent words in the comments are visualized across
three distinct groups:

1.  comments on posts that mention "trump" only,

2.  comments on posts that mention "zelensky" only, and

3.  comments on posts that mention both "trump" and "zelensky".

(it is possible to select words that should be omitted in the plots)

```{r}
library(tidyverse)
library(tidytext)

data("stop_words")

plot_top_words_by_topic <- function(dataset, top_n = 20) {
  dataset_name <- deparse(substitute(dataset))
  
  # Custom stopwords
  custom_stopwords <- c("5", "gt", "https", "time", "1", "it’s", "wiki", "i’m", "I’m")
  all_stopwords <- stop_words %>%
    bind_rows(tibble(word = custom_stopwords, lexicon = "custom")) %>%
    distinct(word)

  # Filter groups
  comments_only_trump <- dataset %>%
    filter(str_detect(post_title, regex("trump", ignore_case = TRUE)) &
             !str_detect(post_title, regex("zelensky", ignore_case = TRUE)))
  
  comments_only_zelensky <- dataset %>%
    filter(str_detect(post_title, regex("zelensky", ignore_case = TRUE)) &
             !str_detect(post_title, regex("trump", ignore_case = TRUE)))
  
  comments_both <- dataset %>%
    filter(str_detect(post_title, regex("trump", ignore_case = TRUE)) &
             str_detect(post_title, regex("zelensky", ignore_case = TRUE)))
  
  # Tokenize and count
  words_trump <- comments_only_trump %>%
    select(comment_body) %>%
    unnest_tokens(word, comment_body) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Trump only")
  
  words_zelensky <- comments_only_zelensky %>%
    select(comment_body) %>%
    unnest_tokens(word, comment_body) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Zelensky only")
  
  words_both <- comments_both %>%
    select(comment_body) %>%
    unnest_tokens(word, comment_body) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Both")
  
  top_words_all <- bind_rows(words_trump, words_zelensky, words_both)
  
  # Plot for Trump and Zelensky only
  plot_limited <- top_words_all %>%
    filter(group %in% c("Trump only", "Zelensky only")) %>%
    group_by(group) %>%
    slice_max(n, n = top_n) %>%
    ungroup() %>%
    ggplot(aes(x = reorder_within(word, n, group), y = n, fill = group)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ group, scales = "free_y") +
    scale_x_reordered() +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Trump-Only and Zelensky-Only Posts -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  # Plot for posts mentioning both Trump and Zelensky
  plot_both <- top_words_all %>%
    filter(group == "Both") %>%
    slice_max(n, n = top_n) %>%
    ggplot(aes(x = reorder(word, n), y = n)) +
    geom_col(fill = "gray40") +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Comments on Posts Mentioning Both Trump and Zelensky -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  # Return both plots as a named list
  return(list(
    trump_zelensky_plot = plot_limited,
    both_plot = plot_both
  ))
}
```

```{r}
plot_top_words_by_topic(data_new)
plot_top_words_by_topic(data_pag)
plot_top_words_by_topic(data_trump_new)
plot_top_words_by_topic(data_trump_rel)
plot_top_words_by_topic(data_zelensky_new)
plot_top_words_by_topic(data_zelensky_rel)
plot_top_words_by_topic(data_trump_zelensky_new)
plot_top_words_by_topic(data_trump_zelensky_rel)
plot_top_words_by_topic(data_war_ukraine_new)
plot_top_words_by_topic(data_war_ukraine_rel)
```

#### Frequent words - youtube
```{r}
library(tidyverse)
library(tidytext)

data("stop_words")

plot_top_words_by_topic_youtube <- function(dataset, top_n = 20) {
  dataset_name <- deparse(substitute(dataset))
  
  # 1️⃣ Custom stopwords
  custom_stopwords <- c("5", "gt", "https", "time", "1", "it’s", "wiki", "i’m", "I’m")
  all_stopwords <- stop_words %>%
    bind_rows(tibble(word = custom_stopwords, lexicon = "custom")) %>%
    distinct(word)
  
  # 2️⃣ Basic cleaning: remove empty or short comments
  dataset_clean <- dataset %>%
    filter(!is.na(comment_text), str_length(comment_text) > 2)
  
  # 3️⃣ Split into topic groups based on video_title
  comments_only_trump <- dataset_clean %>%
    filter(str_detect(video_title, regex("\\btrump\\b", ignore_case = TRUE)) &
             !str_detect(video_title, regex("\\bzelensky\\b", ignore_case = TRUE)))
  
  comments_only_zelensky <- dataset_clean %>%
    filter(str_detect(video_title, regex("\\bzelensky\\b", ignore_case = TRUE)) &
             !str_detect(video_title, regex("\\btrump\\b", ignore_case = TRUE)))
  
  comments_both <- dataset_clean %>%
    filter(str_detect(video_title, regex("\\btrump\\b", ignore_case = TRUE)) &
             str_detect(video_title, regex("\\bzelensky\\b", ignore_case = TRUE)))
  
  # 4️⃣ Tokenize and count words for each group
  words_trump <- comments_only_trump %>%
    select(comment_text) %>%
    unnest_tokens(word, comment_text) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Trump only")
  
  words_zelensky <- comments_only_zelensky %>%
    select(comment_text) %>%
    unnest_tokens(word, comment_text) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Zelensky only")
  
  words_both <- comments_both %>%
    select(comment_text) %>%
    unnest_tokens(word, comment_text) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Both")
  
  top_words_all <- bind_rows(words_trump, words_zelensky, words_both)
  
  # 5️⃣ Plot for Trump and Zelensky only
  plot_limited <- top_words_all %>%
    filter(group %in% c("Trump only", "Zelensky only")) %>%
    group_by(group) %>%
    slice_max(n, n = top_n) %>%
    ungroup() %>%
    ggplot(aes(x = reorder_within(word, n, group), y = n, fill = group)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ group, scales = "free_y") +
    scale_x_reordered() +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Trump-Only and Zelensky-Only YouTube Videos -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  # 6️⃣ Plot for videos mentioning both
  plot_both <- top_words_all %>%
    filter(group == "Both") %>%
    slice_max(n, n = top_n) %>%
    ggplot(aes(x = reorder(word, n), y = n)) +
    geom_col(fill = "gray40") +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Comments on Videos Mentioning Both Trump and Zelensky -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  return(list(
    trump_zelensky_plot = plot_limited,
    both_plot = plot_both
  ))
}
```

```{r}
plot_top_words_by_topic_youtube(data_youtube_zelensky_trump_comments_only)
plot_top_words_by_topic_youtube(data_youtube_zelensky_trump_data)
```




## Exploratory analysis NEW

### 1. on separated datasets

#### Check users activity (posts + comments)

Function to visualize the activity (by post or comments) over time:

```{r}
library(dplyr)
library(lubridate)
library(ggplot2)
library(patchwork)

plot_activity_over_time <- function(dataset, source = "reddit") {
  
  dataset_name <- deparse(substitute(dataset))
  
  if (source == "reddit") {
    dataset <- dataset %>%
      mutate(
        comment_created = ymd_hms(comment_created_utc),
        post_created = ymd_hms(post_created_utc)
      )
    
    # Count comments per day
    comments_per_day <- dataset %>%
      mutate(date = as_date(comment_created)) %>%
      count(date, name = "count")
    
    # Count posts per day
    posts_per_day <- dataset %>%
      mutate(date = as_date(post_created)) %>%
      count(date, name = "count")
    
    # Plots
    p_comments <- ggplot(comments_per_day, aes(x = date, y = count)) +
      geom_line(color = "steelblue") +
      labs(
        title = paste("Comments Over Time -", dataset_name),
        x = "Date",
        y = "Number of Comments"
      ) +
      theme_minimal()
    
    p_posts <- ggplot(posts_per_day, aes(x = date, y = count)) +
      geom_line(color = "darkred") +
      labs(
        title = paste("Posts Over Time -", dataset_name),
        x = "Date",
        y = "Number of Posts"
      ) +
      theme_minimal()
    
  } else if (source == "youtube") {
    dataset <- dataset %>%
      mutate(
        comment_created = ymd_hms(comment_published_at),
        video_created = ymd_hms(video_published_at)
      )
    
    # Count comments per day
    comments_per_day <- dataset %>%
      mutate(date = as_date(comment_created)) %>%
      count(date, name = "count")
    
    # Count videos per day
    videos_per_day <- dataset %>%
      mutate(date = as_date(video_created)) %>%
      count(date, name = "count")
    
    # Plots
    p_comments <- ggplot(comments_per_day, aes(x = date, y = count)) +
      geom_line(color = "steelblue") +
      labs(
        title = paste("YouTube Comments Over Time -", dataset_name),
        x = "Date",
        y = "Number of Comments"
      ) +
      theme_minimal()
    
    p_posts <- ggplot(videos_per_day, aes(x = date, y = count)) +
      geom_line(color = "darkred") +
      labs(
        title = paste("YouTube Videos Over Time -", dataset_name),
        x = "Date",
        y = "Number of Videos"
      ) +
      theme_minimal()
  }
  return(p_comments / p_posts + plot_layout(heights = c(1, 1)))
}
```

```{r}
plot_activity_over_time(data_putin_new_p)
plot_activity_over_time(data_putin_rel_p)
plot_activity_over_time(data_trump_new_p)
plot_activity_over_time(data_trump_rel_p)
plot_activity_over_time(data_trump_zelensky_new_p)
plot_activity_over_time(data_trump_zelensky_rel_p)
plot_activity_over_time(data_war_ukraine_new_p)
plot_activity_over_time(data_war_ukraine_rel_p)
plot_activity_over_time(data_zelensky_new_p)
plot_activity_over_time(data_zelensky_rel_p)
plot_activity_over_time(data_russian_inv_new_p)
plot_activity_over_time(data_russian_inv_rel_p)

plot_activity_over_time(data_putin_new_u)
plot_activity_over_time(data_putin_rel_u)
plot_activity_over_time(data_trump_new_u)
plot_activity_over_time(data_trump_rel_u)
plot_activity_over_time(data_trump_zelensky_new_u)
plot_activity_over_time(data_trump_zelensky_rel_u)
plot_activity_over_time(data_war_ukraine_new_u)
plot_activity_over_time(data_war_ukraine_rel_u)
plot_activity_over_time(data_zelensky_new_u)
plot_activity_over_time(data_zelensky_rel_u)
plot_activity_over_time(data_russian_inv_new_u)
plot_activity_over_time(data_russian_inv_rel_u)

plot_activity_over_time(data_putin_new_wn)
plot_activity_over_time(data_putin_rel_wn)
plot_activity_over_time(data_trump_new_wn)
plot_activity_over_time(data_trump_rel_wn)
plot_activity_over_time(data_trump_zelensky_new_wn)
plot_activity_over_time(data_trump_zelensky_rel_wn)
plot_activity_over_time(data_war_ukraine_new_wn)
plot_activity_over_time(data_war_ukraine_rel_wn)
plot_activity_over_time(data_zelensky_new_wn)
plot_activity_over_time(data_zelensky_rel_wn)
plot_activity_over_time(data_russian_inv_new_wn)
plot_activity_over_time(data_russian_inv_rel_wn)

```


```{r}
plot_activity_over_time(data_yt_war_ukraine1, source = "youtube")
plot_activity_over_time(data_yt_war_ukraine2, source = "youtube")
plot_activity_over_time(data_yt_war_ukraine1_comm, source = "youtube")
plot_activity_over_time(data_yt_war_ukraine2_comm, source = "youtube")
plot_activity_over_time(data_yt_zelensky_trump1, source = "youtube")
plot_activity_over_time(data_yt_zelensky_trump2, source = "youtube")
plot_activity_over_time(data_yt_zelensky_trump1_comm, source = "youtube")
plot_activity_over_time(data_yt_zelensky_trump2_comm, source = "youtube")
```


To visualize the distribution on comments per post for both dataset the
boxplots are shown:

```{r}
library(dplyr)
library(ggplot2)
library(purrr)

plot_comments_per_item <- function(..., source = "reddit") {
  
  datasets <- list(...)
  dataset_names <- as.list(substitute(list(...)))[-1L] |> sapply(deparse)
  
  combined_data <- purrr::map2_dfr(datasets, dataset_names, ~ {
    df <- .x
    
    if (source == "reddit") {
      df %>%
        group_by(post_id) %>%
        summarise(n_comments = n(), .groups = "drop") %>%
        mutate(dataset = .y)
    } else {
      df %>%
        group_by(video_id) %>%
        summarise(n_comments = n(), .groups = "drop") %>%
        mutate(dataset = .y)
    }
  })
  
  ggplot(combined_data, aes(x = dataset, y = n_comments, fill = dataset)) +
    geom_boxplot() +
    labs(
      title = ifelse(source == "reddit",
                     "Number of Comments per Post by Dataset",
                     "Number of Comments per Video by Dataset"),
      x = "Dataset",
      y = "Number of Comments"
    ) +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}


plot_comments_per_item(data_trump_new_p, data_trump_rel_p, data_zelensky_new_p, data_zelensky_rel_p, data_trump_zelensky_new_p, data_trump_zelensky_rel_p, data_putin_new_p, data_putin_rel_p, data_war_ukraine_new_p, data_war_ukraine_rel_p, data_russian_inv_new_p, data_russian_inv_rel_p)
plot_comments_per_item(data_trump_new_u, data_trump_rel_u, data_zelensky_new_u, data_zelensky_rel_u, data_trump_zelensky_new_u, data_trump_zelensky_rel_u, data_putin_new_u, data_putin_rel_u, data_war_ukraine_new_u, data_war_ukraine_rel_u, data_russian_inv_new_u, data_russian_inv_rel_u)
plot_comments_per_item(data_trump_new_wn, data_trump_rel_wn, data_zelensky_new_wn, data_zelensky_rel_wn, data_trump_zelensky_new_wn, data_trump_zelensky_rel_wn, data_putin_new_wn, data_putin_rel_wn, data_war_ukraine_new_wn, data_war_ukraine_rel_wn, data_russian_inv_new_wn, data_russian_inv_rel_wn)
plot_comments_per_item(data_yt_war_ukraine1, data_yt_war_ukraine2, data_yt_war_ukraine1_comm, data_yt_war_ukraine2_comm, data_yt_zelensky_trump1, data_yt_zelensky_trump2, data_yt_zelensky_trump1_comm, data_yt_zelensky_trump2_comm, source = "youtube")
```


#### Frequent words - reddit

ON REDDIT The most frequent words in the comments are visualized across
three distinct groups:

1.  comments on posts that mention "trump" only,

2.  comments on posts that mention "zelensky" only, and

3.  comments on posts that mention both "trump" and "zelensky".

(it is possible to select words that should be omitted in the plots)

```{r}
library(tidyverse)
library(tidytext)

data("stop_words")

plot_top_words_by_topic <- function(dataset, top_n = 20) {
  dataset_name <- deparse(substitute(dataset))
  
  # Custom stopwords
  custom_stopwords <- c("5", "gt", "https", "time", "1", "it’s", "wiki", "i’m", "I’m")
  all_stopwords <- stop_words %>%
    bind_rows(tibble(word = custom_stopwords, lexicon = "custom")) %>%
    distinct(word)

  # Filter groups
  comments_only_trump <- dataset %>%
    filter(str_detect(post_title, regex("trump", ignore_case = TRUE)) &
             !str_detect(post_title, regex("zelensky", ignore_case = TRUE)))
  
  comments_only_zelensky <- dataset %>%
    filter(str_detect(post_title, regex("zelensky", ignore_case = TRUE)) &
             !str_detect(post_title, regex("trump", ignore_case = TRUE)))
  
  comments_both <- dataset %>%
    filter(str_detect(post_title, regex("trump", ignore_case = TRUE)) &
             str_detect(post_title, regex("zelensky", ignore_case = TRUE)))
  
  # Tokenize and count
  words_trump <- comments_only_trump %>%
    select(comment_body) %>%
    unnest_tokens(word, comment_body) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Trump only")
  
  words_zelensky <- comments_only_zelensky %>%
    select(comment_body) %>%
    unnest_tokens(word, comment_body) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Zelensky only")
  
  words_both <- comments_both %>%
    select(comment_body) %>%
    unnest_tokens(word, comment_body) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Both")
  
  top_words_all <- bind_rows(words_trump, words_zelensky, words_both)
  
  # Plot for Trump and Zelensky only
  plot_limited <- top_words_all %>%
    filter(group %in% c("Trump only", "Zelensky only")) %>%
    group_by(group) %>%
    slice_max(n, n = top_n) %>%
    ungroup() %>%
    ggplot(aes(x = reorder_within(word, n, group), y = n, fill = group)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ group, scales = "free_y") +
    scale_x_reordered() +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Trump-Only and Zelensky-Only Posts -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  # Plot for posts mentioning both Trump and Zelensky
  plot_both <- top_words_all %>%
    filter(group == "Both") %>%
    slice_max(n, n = top_n) %>%
    ggplot(aes(x = reorder(word, n), y = n)) +
    geom_col(fill = "gray40") +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Comments on Posts Mentioning Both Trump and Zelensky -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  # Return both plots as a named list
  return(list(
    trump_zelensky_plot = plot_limited,
    both_plot = plot_both
  ))
}
```


```{r}
plot_top_words_by_topic(data_putin_new_p)
plot_top_words_by_topic(data_putin_rel_p)
plot_top_words_by_topic(data_trump_new_p)
plot_top_words_by_topic(data_trump_rel_p)
plot_top_words_by_topic(data_trump_zelensky_new_p)
plot_top_words_by_topic(data_trump_zelensky_rel_p)
plot_top_words_by_topic(data_war_ukraine_new_p)
plot_top_words_by_topic(data_war_ukraine_rel_p)
plot_top_words_by_topic(data_zelensky_new_p)
plot_top_words_by_topic(data_zelensky_rel_p)
plot_top_words_by_topic(data_russian_inv_new_p)
plot_top_words_by_topic(data_russian_inv_rel_p)

plot_top_words_by_topic(data_putin_new_u)
plot_top_words_by_topic(data_putin_rel_u)
plot_top_words_by_topic(data_trump_new_u)
plot_top_words_by_topic(data_trump_rel_u)
plot_top_words_by_topic(data_trump_zelensky_new_u)
plot_top_words_by_topic(data_trump_zelensky_rel_u)
plot_top_words_by_topic(data_war_ukraine_new_u)
plot_top_words_by_topic(data_war_ukraine_rel_u)
plot_top_words_by_topic(data_zelensky_new_u)
plot_top_words_by_topic(data_zelensky_rel_u)
plot_top_words_by_topic(data_russian_inv_new_u)
plot_top_words_by_topic(data_russian_inv_rel_u)

plot_top_words_by_topic(data_putin_new_wn)
plot_top_words_by_topic(data_putin_rel_wn)
plot_top_words_by_topic(data_trump_new_wn)
plot_top_words_by_topic(data_trump_rel_wn)
plot_top_words_by_topic(data_trump_zelensky_new_wn)
plot_top_words_by_topic(data_trump_zelensky_rel_wn)
plot_top_words_by_topic(data_war_ukraine_new_wn)
plot_top_words_by_topic(data_war_ukraine_rel_wn)
plot_top_words_by_topic(data_zelensky_new_wn)
plot_top_words_by_topic(data_zelensky_rel_wn)
plot_top_words_by_topic(data_russian_inv_new_wn)
plot_top_words_by_topic(data_russian_inv_rel_wn)


```


#### Frequent words - youtube
```{r}
library(tidyverse)
library(tidytext)

data("stop_words")

plot_top_words_by_topic_youtube <- function(dataset, top_n = 20) {
  dataset_name <- deparse(substitute(dataset))
  
  # 1️⃣ Custom stopwords
  custom_stopwords <- c("5", "gt", "https", "time", "1", "it’s", "wiki", "i’m", "I’m")
  all_stopwords <- stop_words %>%
    bind_rows(tibble(word = custom_stopwords, lexicon = "custom")) %>%
    distinct(word)
  
  # 2️⃣ Basic cleaning: remove empty or short comments
  dataset_clean <- dataset %>%
    filter(!is.na(comment_text), str_length(comment_text) > 2)
  
  # 3️⃣ Split into topic groups based on video_title
  comments_only_trump <- dataset_clean %>%
    filter(str_detect(video_title, regex("\\btrump\\b", ignore_case = TRUE)) &
             !str_detect(video_title, regex("\\bzelensky\\b", ignore_case = TRUE)))
  
  comments_only_zelensky <- dataset_clean %>%
    filter(str_detect(video_title, regex("\\bzelensky\\b", ignore_case = TRUE)) &
             !str_detect(video_title, regex("\\btrump\\b", ignore_case = TRUE)))
  
  comments_both <- dataset_clean %>%
    filter(str_detect(video_title, regex("\\btrump\\b", ignore_case = TRUE)) &
             str_detect(video_title, regex("\\bzelensky\\b", ignore_case = TRUE)))
  
  # 4️⃣ Tokenize and count words for each group
  words_trump <- comments_only_trump %>%
    select(comment_text) %>%
    unnest_tokens(word, comment_text) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Trump only")
  
  words_zelensky <- comments_only_zelensky %>%
    select(comment_text) %>%
    unnest_tokens(word, comment_text) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Zelensky only")
  
  words_both <- comments_both %>%
    select(comment_text) %>%
    unnest_tokens(word, comment_text) %>%
    anti_join(all_stopwords, by = "word") %>%
    count(word, sort = TRUE) %>%
    mutate(group = "Both")
  
  top_words_all <- bind_rows(words_trump, words_zelensky, words_both)
  
  # 5️⃣ Plot for Trump and Zelensky only
  plot_limited <- top_words_all %>%
    filter(group %in% c("Trump only", "Zelensky only")) %>%
    group_by(group) %>%
    slice_max(n, n = top_n) %>%
    ungroup() %>%
    ggplot(aes(x = reorder_within(word, n, group), y = n, fill = group)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ group, scales = "free_y") +
    scale_x_reordered() +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Trump-Only and Zelensky-Only YouTube Videos -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  # 6️⃣ Plot for videos mentioning both
  plot_both <- top_words_all %>%
    filter(group == "Both") %>%
    slice_max(n, n = top_n) %>%
    ggplot(aes(x = reorder(word, n), y = n)) +
    geom_col(fill = "gray40") +
    coord_flip() +
    labs(
      title = paste("Top", top_n, "Words in Comments on Videos Mentioning Both Trump and Zelensky -", dataset_name),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()
  
  return(list(
    trump_zelensky_plot = plot_limited,
    both_plot = plot_both
  ))
}
```

```{r}
plot_top_words_by_topic_youtube(data_yt_war_ukraine1)
plot_top_words_by_topic_youtube(data_yt_war_ukraine2)
plot_top_words_by_topic_youtube(data_yt_war_ukraine1_comm)
plot_top_words_by_topic_youtube(data_yt_war_ukraine2_comm)
plot_top_words_by_topic_youtube(data_yt_zelensky_trump1)
plot_top_words_by_topic_youtube(data_yt_zelensky_trump2)
plot_top_words_by_topic_youtube(data_yt_zelensky_trump1_comm)
plot_top_words_by_topic_youtube(data_yt_zelensky_trump2_comm)
```


### Merge datasets

#### merged reddit

```{r}
library(dplyr)
library(purrr)

merge_reddit_datasets <- function(...) {
  datasets <- list(...)
  
  dataset_names <- as.character(match.call(expand.dots = FALSE)$...)

  create_comment_key <- function(df, source_name) {
    df %>%
      mutate(
        comment_key = paste(post_id, comment_author, comment_body, sep = "_"),
        source_dataset = source_name
      )
  }
  
  datasets_tagged <- map2(datasets, dataset_names, create_comment_key)
  
  merged <- datasets_tagged[[1]]
  
  for (i in 2:length(datasets_tagged)) {
    current <- datasets_tagged[[i]]
    
    new_comments <- current %>%
      filter(!(comment_key %in% merged$comment_key))
    
    merged <- bind_rows(merged, new_comments)
  }
  
  merged <- merged %>% select(-comment_key)
  
  return(merged)
}

data_merged_reddit_OLD <- merge_reddit_datasets(
  data_new,
  data_pag,
  data_trump_new,
  data_trump_rel,
  data_trump_zelensky_new,
  data_trump_zelensky_rel,
  data_war_ukraine_new,
  data_war_ukraine_rel,
  data_zelensky_new,
  data_zelensky_rel
)
```

Some examples of the size of merged datasets:

data_new: n = 19241 data_pag: n = 33600 data_new + data_pag: n = 34692

data_trump_new: n = 17896 data_trump_rel: n = 120171 data_trump_new +
data_trump_rel: n = 133778

data_trump_zelensky_new: n = 19238 data_trump_zelensky_rel: n = 33595
data_trump_zelensky_new + data_trump_zelensky_rel: n = 34685

data_zelensky_new: n = 18622 data_zelensky_rel: n = 41898
data_zelensky_new + data_zelensky_rel: n = 44033

***data_merged_reddit: n = 219913***

```{r}
data_merged_reddit <- merge_reddit_datasets(
  data_putin_new_p,
  data_putin_new_u,
  data_putin_new_wn,
  data_putin_rel_p,
  data_putin_rel_u,
  data_putin_rel_wn,
  data_russian_inv_new_p,
  data_russian_inv_new_u,
  data_russian_inv_new_wn,
  data_russian_inv_rel_p,
  data_russian_inv_rel_u,
  data_russian_inv_rel_wn,
  data_trump_new_p,
  data_trump_new_u,
  data_trump_new_wn,
  data_trump_rel_p,
  data_trump_rel_u,
  data_trump_rel_wn,
  data_trump_zelensky_new_p,
  data_trump_zelensky_new_u,
  data_trump_zelensky_new_wn,
  data_trump_zelensky_rel_p,
  data_trump_zelensky_rel_u,
  data_trump_zelensky_rel_wn,
  data_war_ukraine_new_p,
  data_war_ukraine_new_u,
  data_war_ukraine_new_wn,
  data_war_ukraine_rel_p,
  data_war_ukraine_rel_u,
  data_war_ukraine_rel_wn,
  data_zelensky_new_p,
  data_zelensky_new_u,
  data_zelensky_new_wn,
  data_zelensky_rel_p,
  data_zelensky_rel_u,
  data_zelensky_rel_wn
)

```

```{r}
data_merged_reddit_rel <- merge_reddit_datasets(
  data_putin_rel_p,
  data_putin_rel_u,
  data_putin_rel_wn,
  data_russian_inv_rel_p,
  data_russian_inv_rel_u,
  data_russian_inv_rel_wn,
  data_trump_rel_p,
  data_trump_rel_u,
  data_trump_rel_wn,
  data_trump_zelensky_rel_p,
  data_trump_zelensky_rel_u,
  data_trump_zelensky_rel_wn,
  data_war_ukraine_rel_p,
  data_war_ukraine_rel_u,
  data_war_ukraine_rel_wn,
  data_zelensky_rel_p,
  data_zelensky_rel_u,
  data_zelensky_rel_wn
)

```


### merged youtube
```{r}
library(dplyr)
library(purrr)

merge_youtube_datasets <- function(...) {
  datasets <- list(...)
  
  # Get dataset names for tracking source
  dataset_names <- as.character(match.call(expand.dots = FALSE)$...)
  
  # Function to create unique comment key for YouTube
  create_comment_key <- function(df, source_name) {
    df %>%
      mutate(
        comment_key = paste(video_id, comment_author, comment_text, sep = "_"),
        source_dataset = source_name
      )
  }
  
  # Apply key creation to all datasets
  datasets_tagged <- map2(datasets, dataset_names, create_comment_key)
  
  # Initialize merged dataset with the first dataset
  merged <- datasets_tagged[[1]]
  
  # Sequentially merge unique comments from other datasets
  for (i in 2:length(datasets_tagged)) {
    current <- datasets_tagged[[i]]
    
    new_comments <- current %>%
      filter(!(comment_key %in% merged$comment_key))
    
    merged <- bind_rows(merged, new_comments)
  }
  
  # Remove the helper key
  merged <- merged %>% select(-comment_key)
  
  return(merged)
}

data_merged_youtube_old <- merge_youtube_datasets(
  data_youtube_zelensky_trump_comments_only,
  data_youtube_zelensky_trump_data
)

data_merged_youtube <- merge_youtube_datasets(
  data_yt_war_ukraine1,
  data_yt_war_ukraine2,
  data_yt_war_ukraine1_comm,
  data_yt_war_ukraine2_comm,
  data_yt_zelensky_trump1,
  data_yt_zelensky_trump2,
  data_yt_zelensky_trump1_comm,
  data_yt_zelensky_trump2_comm
)
```

```{r}
dim(data_merged_youtube)
```
#### Check distribution of scores for posts and comments - reddit

```{r}
library(dplyr)
library(ggplot2)
library(patchwork)

plot_post_comment_scores <- function(dataset, dataset_name = "Dataset") {
  
  post_scores <- dataset %>%
    distinct(post_id, post_score, post_title)
  
  p_posts <- ggplot(post_scores, aes(x = post_score)) +
    geom_histogram(binwidth = 1, fill = "darkred", color = "black") +
    labs(
      title = paste("Distribution of Post Scores -", dataset_name),
      x = "Post Score",
      y = "Number of Posts"
    ) +
    theme_minimal()
  
  p_comments <- ggplot(dataset, aes(x = comment_score)) +
    geom_histogram(binwidth = 1, fill = "steelblue", color = "black") +
    labs(
      title = paste("Distribution of Comment Scores -", dataset_name),
      x = "Comment Score",
      y = "Number of Comments"
    ) +
    theme_minimal()
  
  return(p_posts / p_comments)
}
```

```{r}
plot_post_comment_scores(data_merged_reddit, "Reddit Merged")
```

#### Filter dataset on "score", "date" (+ if removed)

```{r}
library(dplyr)
library(lubridate)

filter_reddit_data <- function(data,
                               min_post_score = 5,
                               min_comment_score = 5,
                               start_date = "2024-09-01") {
  data %>%
    mutate(comment_datetime = ymd_hms(comment_created_utc)) %>%
    filter(
      post_score > min_post_score,
      comment_score > min_comment_score,
      comment_body != "[removed]",
      comment_datetime >= ymd(start_date)
    )
}

# old dataset
filtered_data_reddit_old <- filter_reddit_data(
  data_merged_reddit_OLD,
  min_post_score = 5,
  min_comment_score = 5,
  start_date = "2024-09-01"
)

dim(filtered_data_reddit_old)

# new dataset
filtered_data_reddit <- filter_reddit_data(
  data_merged_reddit,
  min_post_score = 5,
  min_comment_score = 5,
  start_date = "2024-09-01"
)

dim(filtered_data_reddit)

# only rel
filtered_data_reddit_rel <- filter_reddit_data(
  data_merged_reddit_rel,
  min_post_score = 5,
  min_comment_score = 5,
  start_date = "2024-09-01"
)

dim(filtered_data_reddit_rel)
```

```{r}
plot_post_comment_scores(filtered_data_reddit, "Filtered Reddit Merged")
plot_post_comment_scores(filtered_data_reddit_rel, "Filtered Reddit Relevant Merged")
```

#### Filter youtube data on "date"
```{r}
library(dplyr)
library(lubridate)

filter_youtube_data <- function(data,
                                start_date = "2024-09-01") {
  data %>%
    mutate(comment_datetime = ymd_hms(comment_published_at)) %>%
    filter(comment_datetime >= ymd(start_date))
}

# old dataset
filtered_youtube_old <- filter_youtube_data(
  data_merged_youtube_old,
  start_date = "2024-09-01"
)

dim(filtered_youtube_old)

# new dataset
filtered_youtube <- filter_youtube_data(
  data_merged_youtube,
  start_date = "2024-09-01"
)

dim(filtered_youtube)
```
## Sentiment Analysis

The observations from YouTube were divided into four categories based on the content of the video titles: videos mentioning only “Trump,” videos mentioning only “Zelensky,” videos mentioning both, and videos mentioning neither. This classification provides a clear framework for analyzing patterns of viewer engagement and sentiment toward each figure or event. By distinguishing the topics at the title level, we ensure that the sentiment analysis and engagement trends are interpreted with respect to the intended subject of the video.

```{r}
categorize_youtube_videos <- function(data) {
  
  # 1️⃣ Crea categorie uniche per video
  video_categories <- data %>%
    distinct(video_id, video_title) %>%
    mutate(
      title_lower = tolower(video_title),
      has_trump = str_detect(title_lower, "\\btrump\\b"),
      has_zelensky = str_detect(title_lower, "\\bzelensky\\b"),
      category = case_when(
        has_trump & !has_zelensky ~ "Only Trump",
        !has_trump & has_zelensky ~ "Only Zelensky",
        has_trump & has_zelensky ~ "Both",
        TRUE ~ "None"
      )
    ) %>%
    select(video_id, category)
  
  # 2️⃣ Unisci categorie con tutti i commenti
  merged_data <- data %>%
    left_join(video_categories, by = "video_id")
  
  # 3️⃣ Ritorna tutti i dataset in una lista
  return(list(
    Only_Trump    = merged_data %>% filter(category == "Only Trump"),
    Only_Zelensky = merged_data %>% filter(category == "Only Zelensky"),
    Both          = merged_data %>% filter(category == "Both"),
    None          = merged_data %>% filter(category == "None"),
    Merged        = merged_data  # opzionale: dataset completo con categoria
  ))
}

result <- categorize_youtube_videos(filtered_youtube_old)
filtered_youtube_old <- result$Merged

result <- categorize_youtube_videos(filtered_youtube)
filtered_youtube <- result$Merged
```

The observations from Reddit were divided into four categories based on the content of the post titles: posts mentioning only “Trump,” posts mentioning only “Zelensky,” posts mentioning both names, and posts mentioning neither. This categorization allows us to analyze the dynamics of user engagement and sentiment in a more structured way, identifying discussions specifically centered on Trump, Zelensky, their interactions, or unrelated topics. By separating the data into these groups, it becomes possible to investigate how sentiment and activity vary depending on the focus of the conversation.

```{r}
library(dplyr)
library(stringr)

categorize_posts <- function(dataset) {
  
  # 1️⃣ Categorizza i post
  post_categories <- dataset %>%
    distinct(post_id, post_title) %>%
    mutate(
      title_lower = tolower(post_title),
      has_trump = str_detect(title_lower, "\\btrump\\b"),
      has_zelensky = str_detect(title_lower, "\\bzelensky\\b"),
      category = case_when(
        has_trump & !has_zelensky ~ "Only Trump",
        !has_trump & has_zelensky ~ "Only Zelensky",
        has_trump & has_zelensky ~ "Both",
        TRUE ~ "None"
      )
    ) %>%
    select(post_id, category)
  
  # 2️⃣ Join col dataset completo
  merged_with_category <- dataset %>%
    left_join(post_categories, by = "post_id")
  
  # 3️⃣ Crea i 4 dataset
  dataset_only_trump    <- merged_with_category %>% filter(category == "Only Trump")
  dataset_only_zelensky <- merged_with_category %>% filter(category == "Only Zelensky")
  dataset_both          <- merged_with_category %>% filter(category == "Both")
  dataset_none          <- merged_with_category %>% filter(category == "None")
  
  # 🔹 Output come lista
  return(list(
    merged_dataset = merged_with_category,
    only_trump = dataset_only_trump,
    only_zelensky = dataset_only_zelensky,
    both = dataset_both,
    none = dataset_none
  ))
}

result_old <- categorize_posts(filtered_data_reddit_old)
filtered_data_reddit_old <- result_old$merged_dataset

result <- categorize_posts(filtered_data_reddit)
filtered_data_reddit <- result$merged_dataset
```

```{r}
plot_activity_over_time(filtered_data_reddit)
```

To analyze public sentiment toward Trump and Zelensky over time, each Reddit comment was first labeled with a sentiment target, identifying whether it referred primarily to Trump, Zelensky, both (Mixed), or neither. The sentiment of each comment was then computed using the sentimentr package and aggregated on a daily basis to capture temporal trends. Two types of trends were calculated: a simple daily average and a weighted sentiment, where comments are weighted by the logarithmic combination of their own score and the associated post score, emphasizing more relevant and visible discussions. By plotting both unweighted and weighted daily sentiment, the analysis provides insights into how public opinion evolved over time and how highly-engaged discussions may have influenced the perception of the two political figures during key events of the war.

```{r}
library(dplyr)
library(stringr)
library(lubridate)
library(sentimentr)
library(ggplot2)

analyze_sentiment_reddit <- function(data, plot_results = TRUE) {
  
  # 1️⃣ Label each comment with the sentiment target
  comments_labeled <- data %>%
    mutate(
      comment_lower = tolower(comment_body),
      mentions_trump    = str_detect(comment_lower, "\\btrump\\b"),
      mentions_zelensky = str_detect(comment_lower, "\\bzelensky\\b"),
      sentiment_target = case_when(
        category == "Only Trump" ~ "Trump",
        category == "Only Zelensky" ~ "Zelensky",
        mentions_trump & !mentions_zelensky ~ "Trump",
        !mentions_trump & mentions_zelensky ~ "Zelensky",
        mentions_trump & mentions_zelensky ~ "Mixed",
        TRUE ~ NA_character_
      )
    ) %>%
    filter(!is.na(sentiment_target))  # keep only comments with a clear target
  
  # 2️⃣ Compute sentiment for each comment
  sentiment_scores <- sentiment(comments_labeled$comment_body) %>%
    group_by(element_id) %>%
    summarise(sentiment = mean(sentiment, na.rm = TRUE), .groups = "drop")
  
  # 3️⃣ Add sentiment scores back to the dataset
  comments_with_sentiment <- comments_labeled %>%
    mutate(row_id = row_number()) %>%
    left_join(
      sentiment_scores,
      by = c("row_id" = "element_id")
    )
  
  # 4️⃣ Aggregate sentiment by day and target
  daily_sentiment <- comments_with_sentiment %>%
    mutate(date = as_date(comment_created_utc)) %>%
    group_by(date, sentiment_target) %>%
    summarise(
      mean_sentiment = mean(sentiment, na.rm = TRUE),
      n_comments = n(),
      .groups = "drop"
    )
  
  weighted_daily_sentiment <- comments_with_sentiment %>%
    mutate(
      date = as_date(comment_created_utc),
      weight = log1p(comment_score) + log1p(post_score)
    ) %>%
    group_by(date, sentiment_target) %>%
    summarise(
      weighted_mean_sentiment = weighted.mean(sentiment, w = weight, na.rm = TRUE),
      total_comments = n(),
      .groups = "drop"
    )
  
  # 5️⃣ Optional plots
  if (plot_results) {
    p1 <- ggplot(daily_sentiment, aes(x = date, y = mean_sentiment, color = sentiment_target)) +
      geom_line() +
      geom_point(size = 0.8) +
      labs(
        title = "Daily Sentiment Over Time",
        x = "Date",
        y = "Average Sentiment",
        color = "Target"
      ) +
      theme_minimal()
    
    p2 <- ggplot(weighted_daily_sentiment, aes(x = date, y = weighted_mean_sentiment, color = sentiment_target)) +
      geom_line() +
      geom_point(size = 0.8) +
      labs(
        title = "Weighted Daily Sentiment Over Time",
        x = "Date",
        y = "Weighted Average Sentiment",
        color = "Target"
      ) +
      theme_minimal()
    
    print(p1)
    print(p2)
  }
  
  # 6️⃣ Return results as a list
  return(list(
    comments_with_sentiment = comments_with_sentiment,
    daily_sentiment = daily_sentiment,
    weighted_daily_sentiment = weighted_daily_sentiment
  ))
}

```

```{r}
result_sentiment_old <- analyze_sentiment_reddit(filtered_data_reddit_old, plot_results = TRUE)
```

```{r}
result_sentiment <- analyze_sentiment_reddit(filtered_data_reddit, plot_results = TRUE)
```


For YouTube comments, a similar procedure was applied to measure how sentiment toward Trump and Zelensky evolved over time. Each comment was first examined to determine its sentiment target—Trump, Zelensky, or Mixed—based on both the predefined post categories and the presence of the two names within the comment text. Sentiment scores were then computed using the sentimentr package and averaged at the daily level, enabling the visualization of trends in public opinion across time. This approach provides insight into how audiences on YouTube discussed and emotionally reacted to key political events involving Trump and Zelensky, highlighting shifts in sentiment that could reflect reactions to war-related developments or viral political moments.

```{r}
library(dplyr)
library(stringr)
library(lubridate)
library(sentimentr)
library(ggplot2)

analyze_sentiment_yt <- function(dataset) {
  
  # 1️⃣ Label each comment with the sentiment target
  yt_comments_labeled <- dataset %>%
    mutate(
      comment_lower = tolower(comment_text),
      mentions_trump    = str_detect(comment_lower, "\\btrump\\b"),
      mentions_zelensky = str_detect(comment_lower, "\\bzelensky\\b"),
      sentiment_target = case_when(
        category == "Only Trump" ~ "Trump",
        category == "Only Zelensky" ~ "Zelensky",
        mentions_trump & !mentions_zelensky ~ "Trump",
        !mentions_trump & mentions_zelensky ~ "Zelensky",
        mentions_trump & mentions_zelensky ~ "Mixed",
        TRUE ~ NA_character_
      )
    ) %>%
    filter(!is.na(sentiment_target))  # Keep only comments with a clear target

  # 2️⃣ Compute sentiment for each comment and aggregate by comment
  yt_sentiment_scores <- sentiment(yt_comments_labeled$comment_text) %>%
    group_by(element_id) %>%
    summarise(sentiment = mean(sentiment, na.rm = TRUE), .groups = "drop")

  # 3️⃣ Add sentiment scores back to the dataset
  yt_comments_with_sentiment <- yt_comments_labeled %>%
    mutate(row_id = row_number()) %>%
    left_join(
      yt_sentiment_scores,
      by = c("row_id" = "element_id")
    )

  # 4️⃣ Aggregate sentiment by day and target
  yt_daily_sentiment <- yt_comments_with_sentiment %>%
    mutate(date = as_date(comment_published_at)) %>%
    group_by(date, sentiment_target) %>%
    summarise(
      mean_sentiment = mean(sentiment, na.rm = TRUE),
      n_comments = n(),
      .groups = "drop"
    )

  # 5️⃣ Plot daily sentiment trends
  plot <- ggplot(yt_daily_sentiment, aes(x = date, y = mean_sentiment, color = sentiment_target)) +
    geom_line() +
    geom_point(size = 0.8) +
    labs(
      title = "Daily Sentiment Over Time (YouTube)",
      x = "Date",
      y = "Average Sentiment",
      color = "Target"
    ) +
    theme_minimal()
  print(plot)

  return(list(
    labeled_comments = yt_comments_with_sentiment,
    daily_sentiment = yt_daily_sentiment,
    plot = plot
  ))
}
```


```{r}
yt_result_old <- analyze_sentiment_yt(filtered_youtube_old)
```

```{r}
yt_result <- analyze_sentiment_yt(filtered_youtube)
```

## Analysis of sentiment with respect to war

### Reddit

The sentiment analysis of Reddit comments related to the Ukraine war was conducted by first selecting comments that contained at least two war-related keywords, such as “Ukraine,” “Russia,” “war,” or “Zelensky.” Sentiment scores were computed for each comment using the sentimentr package and aggregated on a daily basis to capture fluctuations in public opinion. Two metrics were considered: the simple daily average sentiment and a weighted sentiment, where each comment was weighted by the logarithmic sum of its post and comment score, emphasizing the most relevant and impactful discussions. This dual approach allows the analysis to distinguish between overall public sentiment and sentiment driven by highly engaged or influential conversations.

```{r}
library(dplyr)
library(stringr)
library(lubridate)
library(ggplot2)
library(syuzhet)  # Per la funzione sentiment()

analyze_war_sentiment <- function(dataset) {
  # 1️⃣ Parole chiave
  ukraine_war_keywords <- c(
    "ukraine", "ucraina", "russia", "putin", "moscow", "mosca",
    "kyiv", "kiev", "donbas", "donbass", "crimea",
    "invasion", "invad", "war", "guerra", "frontline",
    "nato", "zelensky", "kremlin"
  )
  
  # 2️⃣ Filtra commenti con almeno 2 keyword
  comments_war <- dataset %>%
    mutate(
      comment_lower = tolower(comment_body),
      n_keywords_found = str_count(comment_lower, paste(ukraine_war_keywords, collapse = "|"))
    ) %>%
    filter(n_keywords_found >= 2)
  
  # 3️⃣ Calcola sentiment
  sentiment_scores <- sentiment(comments_war$comment_body) %>%
    group_by(element_id) %>%
    summarise(sentiment = mean(sentiment, na.rm = TRUE), .groups = "drop")
  
  # 4️⃣ Join sentiment ai commenti
  comments_war_sentiment <- comments_war %>%
    mutate(row_id = row_number()) %>%
    left_join(sentiment_scores, by = c("row_id" = "element_id"))
  
  # 5️⃣ Sentiment giornaliero
  daily_sentiment <- comments_war_sentiment %>%
    mutate(date = as_date(comment_created_utc)) %>%
    group_by(date) %>%
    summarise(
      mean_sentiment = mean(sentiment, na.rm = TRUE),
      n_comments = n(),
      .groups = "drop"
    )
  
  weighted_daily_sentiment <- comments_war_sentiment %>%
    mutate(
      date = as_date(comment_created_utc),
      weight = log1p(comment_score) + log1p(post_score)
    ) %>%
    group_by(date) %>%
    summarise(
      weighted_mean_sentiment = weighted.mean(sentiment, w = weight, na.rm = TRUE),
      n_comments = n(),
      .groups = "drop"
    )
  
  # 6️⃣ Grafici
  p1 <- ggplot(daily_sentiment, aes(x = date, y = mean_sentiment)) +
    geom_line(color = "steelblue") +
    geom_point(size = 0.8, color = "steelblue") +
    labs(
      title = "Daily Sentiment on Ukraine War (Reddit)",
      x = "Date",
      y = "Average Sentiment"
    ) +
    theme_minimal()
  
  p2 <- ggplot(weighted_daily_sentiment, aes(x = date, y = weighted_mean_sentiment)) +
    geom_line(color = "darkred") +
    geom_point(size = 0.8, color = "darkred") +
    labs(
      title = "Weighted Daily Sentiment on Ukraine War (Reddit)",
      x = "Date",
      y = "Weighted Average Sentiment"
    ) +
    theme_minimal()
  
  # 🔹 Output come lista
  return(list(
    comments_war_sentiment = comments_war_sentiment,
    daily_sentiment = daily_sentiment,
    weighted_daily_sentiment = weighted_daily_sentiment,
    plot_daily = p1,
    plot_weighted_daily = p2
  ))
}
```

```{r}
sentiment_war_old <- analyze_war_sentiment(filtered_data_reddit_old)
sentiment_war_old$plot_daily
sentiment_war_old$plot_weighted_daily

```

```{r}
sentiment_war <- analyze_war_sentiment(filtered_data_reddit)
sentiment_war$plot_daily
sentiment_war$plot_weighted_daily
```

### Youtube
```{r}
library(dplyr)
library(stringr)
library(lubridate)
library(sentimentr)
library(ggplot2)

analyze_war_sentiment_yt <- function(dataset) {
  # 1️⃣ Define Ukraine war keywords
  ukraine_war_keywords <- c(
    "ukraine", "ucraina", "russia", "putin", "moscow", "mosca",
    "kyiv", "kiev", "donbas", "donbass", "crimea",
    "invasion", "invad", "war", "guerra", "frontline",
    "nato", "zelensky", "kremlin"
  )
  
  # 2️⃣ Filter comments mentioning the war (>=2 keywords)
  comments_war <- dataset %>%
    mutate(
      comment_lower = tolower(comment_text),
      n_keywords_found = str_count(comment_lower, paste(ukraine_war_keywords, collapse = "|"))
    ) %>%
    filter(n_keywords_found >= 2)
  
  # 3️⃣ Compute sentiment (optimized with get_sentences)
  sentences <- get_sentences(comments_war$comment_text)
  sentiment_scores <- sentiment(sentences) %>%
    group_by(element_id) %>%
    summarise(sentiment = mean(sentiment, na.rm = TRUE), .groups = "drop")
  
  # 4️⃣ Merge sentiment back
  comments_with_sentiment <- comments_war %>%
    mutate(row_id = row_number()) %>%
    left_join(sentiment_scores, by = c("row_id" = "element_id"))
  
  # 5️⃣ Compute daily average sentiment
  daily_sentiment <- comments_with_sentiment %>%
    mutate(date = as_date(comment_published_at)) %>%
    group_by(date) %>%
    summarise(
      mean_sentiment = mean(sentiment, na.rm = TRUE),
      n_comments = n(),
      .groups = "drop"
    )
  
  # 6️⃣ Plot
  plot <- ggplot(daily_sentiment, aes(x = date, y = mean_sentiment)) +
    geom_line(color = "darkred") +
    geom_point(size = 0.8, color = "darkred") +
    labs(
      title = "Daily Sentiment on Ukraine War (YouTube)",
      x = "Date",
      y = "Average Sentiment"
    ) +
    theme_minimal()
  print(plot)
  
  # 7️⃣ Return both dataset and plot
  return(list(
    comments_with_sentiment = comments_with_sentiment,
    daily_sentiment = daily_sentiment,
    plot = plot
  ))
}
```

```{r}
sentiment_war_yt_old <- analyze_war_sentiment_yt(filtered_youtube_old)
```

```{r}
sentiment_war_yt <- analyze_war_sentiment_yt(filtered_youtube)
```

